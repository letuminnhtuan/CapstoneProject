{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:59:56.781044Z",
     "start_time": "2024-04-25T22:59:52.992482Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import regex\n",
    "import warnings\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchmetrics.functional.text import char_error_rate\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wandb.login(key='cb54d60f74230aa62ccce17a9c1718368ad7183e', relogin=True)\n",
    "wandb.init(project='datn', reinit=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e953c3d9b359f1a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Vocabulary & Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba1cd3974f76eeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, vocab_file, seq_length):\n",
    "        self.vocab_file = vocab_file\n",
    "        self.seq_length = seq_length\n",
    "        self.characters = set()\n",
    "        self.string_to_index = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \" \": 3}\n",
    "        self.index_to_string = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \" \"}\n",
    "\n",
    "    def build_vocab(self):\n",
    "        with open(self.vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "                self.characters.add(line)\n",
    "            f.close()\n",
    "        for index, token in enumerate(set(self.characters)):\n",
    "            self.string_to_index[token] = index + 4\n",
    "            self.index_to_string[index + 4] = token\n",
    "\n",
    "    def vectorize_text(self, sentence, add_special_token=True):\n",
    "        tokens = regex.findall(r'\\X', sentence)\n",
    "        vectors = []\n",
    "        if add_special_token:\n",
    "            vectors.append(self.string_to_index[\"<start>\"])\n",
    "        for token in tokens:\n",
    "            vectors.append(self.string_to_index[token])\n",
    "        if add_special_token:\n",
    "            vectors.append(self.string_to_index[\"<end>\"])\n",
    "        n = self.seq_length - len(vectors)\n",
    "        if n > 0:\n",
    "            vectors.extend([self.string_to_index[\"<pad>\"] for _ in range(n)])\n",
    "        return vectors\n",
    "\n",
    "    def convert_text(self, vectors):\n",
    "        texts = [self.index_to_string[vector] for vector in vectors]\n",
    "        return texts\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_name, file_path, vocab_file, seq_length, image_size):\n",
    "        self.folder_name = folder_name\n",
    "        self.file_path = file_path\n",
    "        self.vocab_file = vocab_file\n",
    "        self.seq_length = seq_length\n",
    "        self.image_size = image_size\n",
    "        # Get input and output\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                image_name, label = line.rstrip().split('--------')\n",
    "                self.input.append(image_name)\n",
    "                self.output.append(label)\n",
    "        # Build vocabulary.txt\n",
    "        self.vocab = Vocabulary(self.vocab_file, self.seq_length)\n",
    "        self.vocab.build_vocab()\n",
    "        # Transform\n",
    "        self.transform = v2.Compose([\n",
    "            v2.PILToTensor(),\n",
    "            v2.ToDtype(torch.float)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image\n",
    "        path = self.input[index]\n",
    "        image_path = os.path.join(self.folder_name, path)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = self.transform(image)\n",
    "        label = self.output[index]\n",
    "        vector_label = self.vocab.vectorize_text(label)\n",
    "        return image_tensor, torch.Tensor(vector_label).int()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:59:58.609235Z",
     "start_time": "2024-04-25T22:59:58.590295Z"
    }
   },
   "id": "552b1deb6b467daa",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ecaaa2e301188ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        return out\n",
    "    \n",
    "class Positional_Encoding(torch.nn.Module):\n",
    "    def __init__(self, seq_length, n_dim):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.n_dim = n_dim\n",
    "\n",
    "    def forward(self):\n",
    "        # positional vector\n",
    "        position_encode = torch.zeros((self.seq_length, self.n_dim))\n",
    "        for pos in range(self.seq_length):\n",
    "            for i in range(0, self.n_dim, 2):\n",
    "                position_encode[pos, i] = math.sin(pos / (10000 ** (2 * i / self.n_dim)))\n",
    "                position_encode[pos, i+1] = math.cos(pos / (10000 ** (2 * i / self.n_dim)))\n",
    "        return position_encode\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_head, n_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_dim = n_dim\n",
    "        self.n_dim_each_head = int(self.n_dim / self.n_head)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # init query, key, value\n",
    "        self.flat = torch.nn.Flatten(-2)\n",
    "        self.query_matrix = torch.nn.Linear(self.n_dim_each_head, self.n_dim_each_head, bias=False)\n",
    "        self.key_matrix = torch.nn.Linear(self.n_dim_each_head, self.n_dim_each_head, bias=False)\n",
    "        self.value_matrix = torch.nn.Linear(self.n_dim_each_head, self.n_dim_each_head, bias=False)\n",
    "        self.output_matrix = torch.nn.Linear(self.n_dim_each_head * self.n_head, self.n_dim_each_head * self.n_head, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):  # (batch_size, seq_length, n_dim)\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        seq_length_query = query.size(1)\n",
    "        # divide head => (batch_size, seq_length, n_head, n_dim_each_head)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_head, self.n_dim_each_head)\n",
    "        key = key.view(batch_size, seq_length, self.n_head, self.n_dim_each_head)\n",
    "        value = value.view(batch_size, seq_length, self.n_head, self.n_dim_each_head)\n",
    "        q = self.query_matrix(query)\n",
    "        k = self.key_matrix(key)\n",
    "        v = self.value_matrix(value)\n",
    "        # transpose => (batch_size, n_head, seq_length, n_dim_each_head)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # -------------------------- Compute MultiHead-Attention --------------------------\n",
    "        \"\"\"\n",
    "        - Step 1: compute matmul(q, k^T)\n",
    "        - Step 2: scale with sqrt(n_dim)\n",
    "        - Step 3: compute softmax => matrix A\n",
    "        - Step 4: compute matmul of matrix A and value matrix\n",
    "        - Step 5: concatenate matrix => matrix Z\n",
    "        - Step 4: compute matmul of matrix Z and matrix W0\n",
    "        \"\"\"\n",
    "        k_T = k.transpose(-1, -2)  # => (batch_size, n_head, n_dim_each_head, seq_length)\n",
    "        product = torch.matmul(q, k_T)  # => (batch_size, n_head, seq_length_query, seq_length)\n",
    "        product = product / math.sqrt(self.n_dim_each_head)\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        product = product.to(self.device)\n",
    "        scores = F.softmax(product, dim=-1)  # => (batch_size, n_head, seq_length_query, seq_length)\n",
    "        scores = torch.matmul(scores, v)  # => (batch_size, n_head, seq_length_query, n_dim_each_head)\n",
    "        scores = scores.transpose(1, 2)  # => (batch_size, seq_length_query, n_head, n_dim_each_head)\n",
    "        scores = self.flat(scores)\n",
    "        output = self.output_matrix(scores)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.551464Z",
     "start_time": "2024-04-25T10:17:00.537456Z"
    }
   },
   "id": "e6a9fe770e86ae42",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, n_head, n_dim, n_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # parameters\n",
    "        self.n_head = n_head\n",
    "        self.n_dim = n_dim\n",
    "        self.n_expansion = n_expansion\n",
    "        # instances\n",
    "        self.multihead = MultiHeadAttention(n_head=self.n_head, n_dim=self.n_dim)\n",
    "        self.norm_attention = torch.nn.LayerNorm(self.n_dim)\n",
    "        self.feedforward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_dim, self.n_expansion * self.n_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.n_expansion * self.n_dim, self.n_dim),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.norm_feedforward = torch.nn.LayerNorm(self.n_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        multihead_vector = self.multihead(query, key, value)\n",
    "        add_norm_vector = self.norm_attention(multihead_vector + query)\n",
    "        feed_forward_vector = self.feedforward(add_norm_vector)\n",
    "        output = self.norm_feedforward(feed_forward_vector + add_norm_vector)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.566925Z",
     "start_time": "2024-04-25T10:17:00.551464Z"
    }
   },
   "id": "941d096d658dae0b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ViT(torch.nn.Module):\n",
    "    def __init__(self, input_chanel, output_chanel, n_head, n_expansion, n_layer):\n",
    "        super(ViT, self).__init__()\n",
    "        # Parameters\n",
    "        self.input_chanel = input_chanel\n",
    "        self.output_chanel = output_chanel\n",
    "        self.n_head = n_head\n",
    "        self.n_expansion = n_expansion\n",
    "        self.n_layer = n_layer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Instance\n",
    "        self.patch_embedding = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=self.input_chanel, out_channels=self.output_chanel, kernel_size=32, stride=32, padding=0),\n",
    "            torch.nn.BatchNorm2d(self.output_chanel),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Flatten(2)\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(self.n_head, self.output_chanel, self.n_expansion)\n",
    "\n",
    "    def add_cls_token(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        cls_token = torch.nn.Parameter(data=torch.zeros(batch_size, 1, self.output_chanel), requires_grad=True).to(self.device)\n",
    "        return torch.concat([cls_token, x], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Input shape: (batch_size, chanel, height, width) \"\"\"\n",
    "        x = self.patch_embedding(x)     # => (batch_size, seq_len, output_chanel)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.add_cls_token(x)       # => (batch_size, seq_len+1, output_chanel)\n",
    "        position = Positional_Encoding(seq_length=x.shape[1], n_dim=self.output_chanel)\n",
    "        x = x + position().requires_grad_(False).to(self.device)\n",
    "        for _ in range(self.n_layer):\n",
    "            x = self.transformer_block(x, x, x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, input_chanel, hidden_dim, output_chanel, n_head, n_expansion, n_layer):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        # Parameters\n",
    "        self.input_chanel = input_chanel\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_chanel = output_chanel\n",
    "        self.n_head = n_head\n",
    "        self.n_expansion = n_expansion\n",
    "        self.n_layer = n_layer\n",
    "        # Instances\n",
    "        self.vit = ViT(self.input_chanel, self.hidden_dim, self.n_head, self.n_expansion, self.n_layer)\n",
    "        self.fc_out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_chanel),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.582889Z",
     "start_time": "2024-04-25T10:17:00.567922Z"
    }
   },
   "id": "fdc24d51816aa15b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, n_head, n_dim, seq_length, vocab_size, n_expansion):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # parameters\n",
    "        self.n_head = n_head\n",
    "        self.n_dim = n_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_expansion = n_expansion\n",
    "        # instance\n",
    "        self.mask_attention = MultiHeadAttention(n_head=self.n_head, n_dim=self.n_dim)\n",
    "        self.norm_mask_attention = torch.nn.LayerNorm(self.n_dim)\n",
    "        self.transformer_block = TransformerBlock(self.n_head, self.n_dim, self.n_expansion)\n",
    "\n",
    "    def forward(self, x, key, value, mask):\n",
    "        masked_output = self.mask_attention(query=x, key=x, value=x, mask=mask)\n",
    "        masked_output = self.norm_mask_attention(x + masked_output)\n",
    "        output = self.transformer_block(query=masked_output, key=key, value=value)\n",
    "        return output\n",
    "\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, n_head, n_dim, seq_length, vocab_size, n_expansion, n_layer):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        # parameters\n",
    "        self.n_head = n_head\n",
    "        self.n_dim = n_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_expansion = n_expansion\n",
    "        self.n_layer = n_layer\n",
    "        # instance\n",
    "        self.embedding = Embedding(vocab_size=self.vocab_size, n_dim=self.n_dim)\n",
    "        self.decoder_layers = DecoderLayer(self.n_head, self.n_dim, self.seq_length, self.vocab_size, self.n_expansion)\n",
    "        self.fc_output = torch.nn.Linear(self.n_dim, self.vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, output_encoder, input_decoder, mask):\n",
    "        embedding_vector = self.embedding(input_decoder)\n",
    "        position = Positional_Encoding(seq_length=embedding_vector.shape[1], n_dim=self.n_dim)\n",
    "        embedding_vector = embedding_vector + position().requires_grad_(False).to(device)\n",
    "        output = self.decoder_layers(x=embedding_vector, key=output_encoder, value=output_encoder, mask=mask)\n",
    "        for _ in range(self.n_layer - 1):\n",
    "            output = self.decoder_layers(x=output, key=output_encoder, value=output_encoder, mask=mask)\n",
    "            output = self.dropout(output)\n",
    "        return self.fc_output(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.598355Z",
     "start_time": "2024-04-25T10:17:00.582889Z"
    }
   },
   "id": "d42b50d49dba0412",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_dim_model,\n",
    "        input_channel_encoder, hidden_dim_encoder, n_head_encoder, n_expansion_encoder, n_layer_encoder,\n",
    "        n_head_decoder, seq_length_decoder, vocab_size_decoder, n_expansion_decoder, n_layer_decoder,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        # Parameters\n",
    "        self.n_dim_model = n_dim_model\n",
    "        self.input_chanel_encoder = input_channel_encoder\n",
    "        self.hidden_dim_encoder = hidden_dim_encoder\n",
    "        self.n_head_encoder = n_head_encoder\n",
    "        self.n_expansion_encoder = n_expansion_encoder\n",
    "        self.n_layer_encoder = n_layer_encoder\n",
    "        self.n_head_decoder = n_head_decoder\n",
    "        self.seq_length_decoder = seq_length_decoder\n",
    "        self.vocab_size_decoder = vocab_size_decoder\n",
    "        self.n_expansion_decoder = n_expansion_decoder\n",
    "        self.n_layer_decoder = n_layer_decoder\n",
    "        # Instances\n",
    "        self.encoder = EncoderBlock(self.input_chanel_encoder, self.hidden_dim_encoder, self.n_dim_model,  self.n_head_encoder, self.n_expansion_encoder, self.n_layer_encoder)\n",
    "        self.decoder = DecoderBlock(self.n_head_decoder, self.n_dim_model, self.seq_length_decoder, self.vocab_size_decoder, self.n_expansion_decoder, self.n_layer_decoder)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mask(seq_input):\n",
    "        batch_size, seq_len = seq_input.shape\n",
    "        mask = torch.tril(torch.ones((seq_len, seq_len))) \n",
    "        return mask.expand(batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "    def forward(self, input_encoder, input_decoder):\n",
    "        output_encoder = self.encoder(input_encoder)\n",
    "        mask = self.create_mask(input_decoder).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        output_decoder = self.decoder(output_encoder, input_decoder, mask)\n",
    "        return output_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.613518Z",
     "start_time": "2024-04-25T10:17:00.598355Z"
    }
   },
   "id": "281f801eee8212a3",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "422da12829c153a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(CustomSchedule, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        arg1 = (step ** -0.5)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return [((self.d_model ** -0.5) * min(arg1, arg2)) for base_lr in self.base_lrs]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.628969Z",
     "start_time": "2024-04-25T10:17:00.613518Z"
    }
   },
   "id": "57c5c33917eeda11",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('inf')  # Initialize with positive infinity for loss\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_metric):\n",
    "        if self.best_metric - current_metric > self.delta:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "        return self.early_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.644451Z",
     "start_time": "2024-04-25T10:17:00.628969Z"
    }
   },
   "id": "636b72f09e9481a2",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def decode_input(input_decoder, vocab):\n",
    "    # input_decoder: (batch_size, seq_len)\n",
    "    index = vocab['<pad>']\n",
    "    vocabulary = list(vocab.keys())\n",
    "    sequences = []\n",
    "    for batch in input_decoder:\n",
    "        text = ''.join([vocabulary[i] for i in batch if i != index])\n",
    "        sequences.append(text)\n",
    "    return sequences\n",
    "\n",
    "def decode_output(output_model, vocab):\n",
    "    # output_model: (batch_size, seq_len, vocab_size)\n",
    "    index = vocab['<pad>']\n",
    "    vocabulary = list(vocab.keys())\n",
    "    sequences = []\n",
    "    for batch in output_model:\n",
    "        output = torch.argmax(batch, dim=-1)\n",
    "        text = ''.join([vocabulary[i] for i in output if i != index])\n",
    "        sequences.append(text)\n",
    "    return sequences\n",
    "\n",
    "def compute_cer(input_decoder, output_model, vocab):\n",
    "    target = decode_input(input_decoder, vocab)\n",
    "    predict = decode_output(output_model, vocab)\n",
    "    with open('log.txt', 'a', encoding='utf-8') as f:\n",
    "        f.write(f'{target} -------- {predict} \\n')\n",
    "    cer = char_error_rate(predict, target)\n",
    "    return cer.item()\n",
    "\n",
    "def compute_recall_precision_f1(input_decoder, output_model):\n",
    "    y = []\n",
    "    y_pred = []\n",
    "    for batch in output_model:\n",
    "        output = torch.argmax(batch, dim=-1)\n",
    "        y_pred.extend([i.cpu() for i in output])\n",
    "    for batch in input_decoder:\n",
    "        y.extend([i.cpu() for i in batch])\n",
    "    recall = recall_score(np.asarray(y), np.asarray(y_pred), average='micro', zero_division=0)\n",
    "    precision = precision_score(np.asarray(y), np.asarray(y_pred), average='micro', zero_division=0)\n",
    "    f1 = f1_score(np.asarray(y), np.asarray(y_pred), average='micro', zero_division=0)\n",
    "    return recall, precision, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.659620Z",
     "start_time": "2024-04-25T10:17:00.644451Z"
    }
   },
   "id": "1e7c56211fb236d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_metrics(input_decoder, output_model, vocab):\n",
    "    input_decoder = input_decoder[:, 1:].contiguous()\n",
    "    output_model = output_model[:, :-1, :].contiguous()\n",
    "    recall, precision, f1 = compute_recall_precision_f1(input_decoder, output_model)\n",
    "    cer = compute_cer(input_decoder, output_model, vocab)\n",
    "    return cer, recall, precision, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:17:00.675585Z",
     "start_time": "2024-04-25T10:17:00.659620Z"
    }
   },
   "id": "8872516ab30dd6b5",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Training model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94dabe95c9c40f20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a. Create dataset & Dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4080f068cb6b39a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_path = '../dataset/augment_data'\n",
    "train_file = '../dataset/augment_labels.txt'\n",
    "vocab_file = '../dataset/vocab.txt'\n",
    "train_ratio = 0.8\n",
    "batch_size = 128\n",
    "seq_length = 192\n",
    "image_size = (640, 640)\n",
    "dataset = CustomDataset(train_path, train_file, vocab_file, seq_length, image_size)\n",
    "n_train = int(train_ratio * len(dataset))\n",
    "n_val = len(dataset) - n_train\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "vocab = dataset.vocab.string_to_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:08.257057Z",
     "start_time": "2024-04-25T23:00:08.183146Z"
    }
   },
   "id": "152e412719397fa3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 687\n",
      "Number of validate examples: 172\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: {}\".format(len(train_loader)))\n",
    "print(\"Number of validate examples: {}\".format(len(val_loader)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:08.792967Z",
     "start_time": "2024-04-25T23:00:08.774480Z"
    }
   },
   "id": "20c27c953bab9faf",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of vocab: 233\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of vocab:\", len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:11.602386Z",
     "start_time": "2024-04-25T23:00:11.585543Z"
    }
   },
   "id": "7fb13f379f76441a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels = []\n",
    "for line in dataset.output:\n",
    "    temp = []\n",
    "    tokens = regex.findall(r'\\X', line)\n",
    "    temp.append(dataset.vocab.string_to_index['<start>'])\n",
    "    temp.extend([dataset.vocab.string_to_index[token] for token in tokens])\n",
    "    temp.append(dataset.vocab.string_to_index['<end>'])\n",
    "    n = seq_length - len(temp)\n",
    "    if n > 0:\n",
    "        for _ in range(n):\n",
    "            temp.append(dataset.vocab.string_to_index['<pad>'])\n",
    "    labels.extend(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:16.620085Z",
     "start_time": "2024-04-25T23:00:13.647397Z"
    }
   },
   "id": "79c16b08a85867b7",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y = labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:18.612859Z",
     "start_time": "2024-04-25T23:00:16.620085Z"
    }
   },
   "id": "63eaa08396018c00",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(233,)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:18.628905Z",
     "start_time": "2024-04-25T23:00:18.612859Z"
    }
   },
   "id": "36df6bf5608f7040",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['<pad>', '<start>', '<end>', ' ', 'ồ', 'ơ', 'ã', 'Y', '2', 'Ẹ', \"'\", 'Ẽ', 'Ỉ', 'U', 'e', 'I', 'p', '6', 'Ỏ', 'ỡ', 'Ủ', 'ộ', 'Ệ', 'Ý', 'Ấ', 'A', 'T', 'Â', 'ấ', 'B', 'Ắ', 'Ị', 'ế', 'C', 'Á', 's', '/', 'n', 'R', 'ọ', '-', 'm', 'Ỹ', 'Ĩ', 'ề', 'd', 'ẹ', 'ẩ', 'Ù', 'Ẫ', 'c', 'ị', 'ẹ', ':', '^', '?', 'Đ', '<', 'Ẩ', 'Ó', 'ư', '1', 'ú', 'Ỵ', 'q', 'í', 'ễ', '%', 'v', 'õ', 'ả', 'J', 'Ổ', 'â', 'V', 'Ế', 'Ự', 'ẵ', 'É', 'i', 'ứ', 'ở', 'Ỳ', 'Ữ', 'ặ', 'ý', '_', 'ê', 'Ê', 'E', 'Ớ', 'Ư', 'Ỷ', '=', '0', 'Ă', 'Ơ', 'Ễ', 't', 'ỵ', 'ạ', 'Ố', 'Ả', 'D', 'M', '>', 'ỹ', 'è', 'Ẵ', ';', 'ệ', '5', '9', '#', 'S', ',', 'ắ', 'Ợ', 'á', 'ò', 'k', '[', 'ặ', 'ừ', 'ẽ', 'Ỗ', 'Ồ', 'L', 'ĩ', 'N', 'ỗ', 'à', '.', 'ằ', 'j', 'Ằ', '{', 'Ọ', 'Ô', 'Ú', '&', 'È', ')', 'ủ', 'Ử', ']', 'ó', '*', '|', 'Ỡ', 'ố', '7', 'Z', 'ô', '!', 'ổ', 'ă', 'Ụ', 'Ẳ', 'h', 'Ẻ', 'w', 'ớ', 'ỏ', 'Ạ', '}', 'é', 'Ầ', '\"', 'K', '8', 'Ở', 'Í', '∈', 'P', 'g̃', 'Ì', 'o', 'Ậ', 'Ể', 'g', 'ì', 'x', 'b', 'Ò', 'Ừ', 'ẫ', '+', 'Ã', 'đ', 'Ộ', 'ỷ', 'ợ', 'W', 'G', 'ẻ', 'ũ', 'ỳ', 'Ề', 'a', 'ỉ', '(', 'y', 'ụ', 'ẳ', 'Ứ', 'ữ', 'Ờ', 'Õ', '3', 'Ặ', 'À', 'ự', 'z', 'À', 'Q', 'O', 'H', 'Ũ', 'ờ', 'ậ', 'f', 'Ằ', '4', 'l', 'X', 'r', 'ù', 'ử', 'ầ', 'u', 'F', 'ể'])\n"
     ]
    }
   ],
   "source": [
    "print(vocab.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:18.644362Z",
     "start_time": "2024-04-25T23:00:18.628905Z"
    }
   },
   "id": "98570b4854224b4e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([5.60930589e-03, 8.24034335e-01, 8.24034335e-01, 9.51417143e-02,\n       5.99767264e+00, 3.94099946e+00, 1.03376971e+01, 1.37146781e+01,\n       6.81192621e+00, 4.11440343e+03, 9.14311874e+01, 6.85733906e+02,\n       8.22880687e+02, 2.18851246e+01, 2.89542817e+00, 9.43670512e+00,\n       1.82537863e+00, 1.24302219e+01, 4.11440343e+03, 2.05720172e+02,\n       1.71433476e+02, 3.82379501e+00, 6.63613457e+01, 2.57150215e+02,\n       1.84502396e+01, 4.88066837e+00, 8.48680576e-01, 1.32722691e+02,\n       6.20573670e+00, 3.28364201e+00, 5.87771919e+02, 6.85733906e+02,\n       4.24165302e+00, 2.47557367e+00, 4.15596306e+01, 2.79321346e+00,\n       1.24678892e+01, 2.12235811e-01, 1.19258071e+01, 9.50208645e+00,\n       2.29854940e+01, 1.27855918e+00, 4.11440343e+03, 2.05720172e+03,\n       6.23394460e+00, 3.78858511e+00, 4.11440343e+02, 2.18851246e+01,\n       8.22880687e+02, 2.05720172e+03, 6.35330981e-01, 5.42797287e+00,\n       1.64576137e+02, 1.53522516e+01, 2.05720172e+03, 6.53079910e+01,\n       3.41161147e+00, 2.05720172e+03, 4.11440343e+03, 1.46942980e+02,\n       1.77574598e+00, 7.95822714e+00, 6.53079910e+00, 4.11440343e+03,\n       9.56838008e+00, 6.53079910e+00, 3.00321419e+01, 9.79619865e+01,\n       3.00760485e+00, 8.22880687e+01, 3.60912582e+00, 1.95923973e+02,\n       3.74036676e+02, 3.05449401e+00, 5.64390046e+00, 1.14288984e+02,\n       1.58246286e+02, 1.24678892e+02, 1.02860086e+03, 4.76369507e-01,\n       1.08273775e+01, 1.56441195e+01, 4.11440343e+03, 2.42023731e+02,\n       5.07951041e+01, 2.36459967e+01, 7.09379902e+01, 3.11225676e+00,\n       1.71433476e+02, 1.39000116e+01, 1.17554384e+02, 5.20810561e+01,\n       4.11440343e+03, 9.79619865e+01, 1.37146781e+01, 8.22880687e+02,\n       2.28577969e+02, 5.87771919e+02, 6.06307609e-01, 1.21011866e+02,\n       3.31272418e+00, 2.05720172e+02, 2.43455824e+01, 6.83455720e+00,\n       4.78975953e+00, 6.85733906e+02, 2.85722461e+01, 9.35091689e+01,\n       4.11440343e+03, 1.32722691e+02, 3.78509975e+00, 1.07145923e+01,\n       1.26208694e+01, 1.02860086e+03, 5.98023755e+00, 2.63069273e+00,\n       8.98341361e+00, 1.05497524e+02, 2.09171501e+00, 1.78112703e+01,\n       3.41727860e+00, 2.42023731e+02, 4.33095098e+01, 1.48000124e+01,\n       3.34504344e+01, 5.87771919e+02, 2.93885960e+02, 3.25249283e+00,\n       1.13657553e+01, 2.01094987e+00, 4.24165302e+01, 1.16522329e+00,\n       3.01642480e+00, 3.51658413e+01, 6.85733906e+02, 2.05720172e+03,\n       1.02860086e+03, 8.22880687e+02, 8.22880687e+01, 6.85733906e+02,\n       5.87771919e+02, 4.11440343e+03, 2.37826788e+01, 1.02603577e+01,\n       1.71433476e+02, 2.42023731e+02, 4.63333720e+00, 6.85733906e+02,\n       2.05720172e+03, 4.11440343e+03, 4.39573016e+00, 1.30615982e+01,\n       4.11440343e+02, 2.88325398e+00, 3.16492572e+02, 1.13033061e+01,\n       8.44846701e+00, 1.17554384e+02, 4.11440343e+03, 2.77550151e-01,\n       2.05720172e+03, 4.78419004e+01, 5.11741721e+00, 3.84523685e+01,\n       1.05497524e+02, 1.02860086e+03, 3.18946003e+01, 2.05720172e+02,\n       1.16555338e+01, 5.47128116e+00, 1.31450589e+01, 1.37146781e+02,\n       2.57150215e+02, 5.87771919e+02, 3.01200837e+00, 4.11440343e+03,\n       2.42023731e+02, 1.17722559e+00, 2.28577969e+02, 1.14288984e+02,\n       5.20283692e-01, 3.97526902e+00, 9.14311874e+00, 3.73357843e+00,\n       1.37146781e+03, 8.22880687e+02, 3.09353642e+01, 3.42866953e+02,\n       5.14300429e+02, 1.82781139e+00, 9.79619865e+01, 5.07951041e+01,\n       8.31192613e+00, 9.35091689e+01, 5.99767264e+00, 8.94435529e+01,\n       2.07798153e+01, 5.01756516e+01, 2.05720172e+02, 8.45888864e-01,\n       3.80963281e+01, 2.37826788e+01, 2.26813861e+00, 1.09425623e+01,\n       9.56838008e+01, 1.87018338e+02, 1.39945695e+01, 6.85733906e+02,\n       4.11440343e+03, 1.02860086e+01, 3.42866953e+02, 1.28575107e+02,\n       1.03117881e+01, 1.02860086e+02, 4.78419004e+01, 8.16349888e+00,\n       1.35788892e+01, 1.66980659e+00, 1.37146781e+03, 5.62076972e+00,\n       5.59022206e+00, 2.78000232e+01, 4.11440343e+03, 1.00107140e+01,\n       1.94626463e+00, 1.00107140e+01, 1.45436671e+00, 1.38532102e+01,\n       1.62624642e+01, 8.92495322e+00, 8.95408799e-01, 5.71444921e+01,\n       6.43881601e+00])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:00:25.944763Z",
     "start_time": "2024-04-25T23:00:25.935287Z"
    }
   },
   "id": "7a429fb4e341b8b0",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b. Create model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5711da7dc6af5611"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_dim_model = 512\n",
    "# --- Encoder Parameters ---\n",
    "input_chanel_encoder = 3\n",
    "hidden_dim_encoder = 768\n",
    "n_head_encoder = 12\n",
    "n_expansion_encoder = 4\n",
    "n_layer_encoder = 12\n",
    "# --- Decoder Parameters ---\n",
    "n_head_decoder = 8\n",
    "seq_length_decoder = seq_length\n",
    "vocab_size_decoder = len(vocab)\n",
    "n_expansion_decoder = 4\n",
    "n_layer_decoder = 6\n",
    "model = Model(n_dim_model, input_chanel_encoder, hidden_dim_encoder, n_head_encoder, n_expansion_encoder, n_layer_encoder,\n",
    "              n_head_decoder, seq_length_decoder, vocab_size_decoder, n_expansion_decoder, n_layer_decoder).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ef795dbe87d9da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14e6f31ed693ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6df61011692cc609",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c. Metrics & Optimizer & Loss function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dec1496ca69f7dab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.Tensor(weights).to(device), ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-5)\n",
    "lr_scheduler = CustomSchedule(optimizer, d_model=n_dim_model, warmup_steps=4000)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, threshold=0.01)\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e59b6e16cc92f1be",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## d. Training Step"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82af3bd634293adf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_cer\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_cer\": []\n",
    "}\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    vocab_size_decoder = len(vocab)\n",
    "    loss_value = 0.0\n",
    "    cer_value = 0.0\n",
    "    recall_value, precision_value, f1_value = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (input_encoder, input_decoder) in enumerate(train_loader):\n",
    "        # Predict\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        input_encoder, input_decoder = input_encoder.to(device), input_decoder.to(device)\n",
    "        output_model = model(input_encoder, input_decoder)\n",
    "        # Compute loss\n",
    "        target = input_decoder[:, 1:].contiguous()\n",
    "        pred = output_model[:, :-1, :].contiguous()\n",
    "        loss = criterion(pred.view(-1, vocab_size_decoder), target.view(-1).long())\n",
    "        loss_value += loss.item()\n",
    "        # Compute metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        cer, recall, precision, f1 = compute_metrics(input_decoder, output_model, vocab)\n",
    "        cer_value += cer\n",
    "        recall_value += recall\n",
    "        precision_value += precision\n",
    "        f1_value += f1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\n",
    "            'loss per step': loss.item(), \n",
    "            'lr': current_lr, \n",
    "            'cer per step': cer,\n",
    "            'recall per step': recall, \n",
    "            'precision per step': precision, \n",
    "            'f1 per step': f1\n",
    "        })\n",
    "    loss_train = loss_value / len(train_loader)\n",
    "    cer_train = cer_value / len(train_loader)\n",
    "    recall_value = recall_value / len(train_loader)\n",
    "    precision_value = precision_value / len(train_loader)\n",
    "    f1_value = f1_value / len(train_loader)\n",
    "    wandb.log({\n",
    "        'train loss': loss_train,\n",
    "        'train cer': cer_train, \n",
    "        'train recall': recall_value,\n",
    "        'train precision': precision_value,\n",
    "        'train f1': f1_value,\n",
    "    })\n",
    "    # Evaluate\n",
    "    loss_value = 0.0\n",
    "    cer_value = 0.0\n",
    "    recall_value, precision_value, f1_value = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (input_encoder, input_decoder) in enumerate(val_loader):\n",
    "            # Predict\n",
    "            input_encoder, input_decoder = input_encoder.to(device), input_decoder.to(device)\n",
    "            output_model = model(input_encoder, input_decoder)\n",
    "            # Compute loss\n",
    "            target = input_decoder[:, 1:].contiguous()\n",
    "            pred = output_model[:, :-1, :].contiguous()\n",
    "            loss = criterion(pred.view(-1, vocab_size_decoder), target.view(-1).long()).item()\n",
    "            cer, recall, precision, f1 = compute_metrics(input_decoder, output_model, vocab)\n",
    "            cer_value += cer\n",
    "            loss_value += loss\n",
    "            recall_value += recall\n",
    "            precision_value += precision\n",
    "            f1_value += f1\n",
    "        loss_val = loss_value / len(val_loader)\n",
    "        cer_val = cer_value / len(val_loader)\n",
    "        recall_value = recall_value / len(val_loader)\n",
    "        precision_value = precision_value / len(val_loader)\n",
    "        f1_value = f1_value / len(val_loader)\n",
    "        wandb.log({\n",
    "            'val loss': loss_val,\n",
    "            'val cer': cer_val, \n",
    "            'val recall': recall_value,\n",
    "            'val precision': precision_value,\n",
    "            'val f1': f1_value,\n",
    "        })\n",
    "    print(f\"Epoch: {epoch + 1} | train_loss: {loss_train:.2f} | val_loss: {loss_val:.2f}\")\n",
    "    if early_stopping(loss_val):\n",
    "        print(\"Early Stopping Training Progress!\")\n",
    "        break\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab': vocab,\n",
    "        'results': results,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, f'../checkpoints/checkpoint-{epoch}.pth.tar')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de58d0715116ef4c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e97067bc5176bb12",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15222148375579c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3fde622b086673",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
